---
title: Some Notes on Executive Dashboards
subtitle: Command & Control & Confusion
layout: blog
---

Why are executive dashboards so bad?

In my consulting work, almost every company has lackluster reporting and dashboards. These days it's less a case of completely missing reporting (though that still happens) but rather the things the executive team are looking at regularly are poorly implemented and lack any real insight into the business.

Most of my consulting work revolves around putting together some kind of strategic plan. It usually boils down to a kind of basic equation, something like this:

"If you invest $$ into activities X, Y and Z then over 2 years we can achieve $$$"

You secure buy-in from the key players, grab the money and get to work.

Oddly though - companies only tend to measure the right hand of the equation. 

Company dashboards are designed around metrics and measurement of results - they're trying to measure ***what has happened***.

Measuring what happened is important, obviously. But it's a bit like driving a car only looking in the rear view mirror...

It's also important, however, to measure what ***is*** happening.

Unfortunately in my consulting work most companies don't have any kind of measurement in place for the left hand side of the equation.

📈📈📈

Maybe there's a blind spot in my consulting. When you put a plan together that says "If you invest $$ into activities X, Y and Z then over 2 years we can achieve $$$" - then there's some kind of assumption, either explicit or implicit that activities X, Y and Z will produce results.

It's kind of obvious that you have to find evidence for this *historically* - I like to show how investing in these activities has paid off previously, or how a similar situation worked out for a similar business.

But perhaps I could better articulate how this *future* investment will play out. Linking to investing to clear outputs that will lead to results. Hmm..

📈📈📈

The book Working Backwards explores this idea - Amazon calls them "input metrics" or "controllable input metrics". From the book:

<blockquote class="quoteback" darkmode="" data-title="Working%20Backwards" data-author="Cedric Chin" cite="https://commoncog.com/blog/working-backwards/">
The simple answer is that <em>we are not taught to think like this</em>. When people say “be more data driven”, we immediately assume, “oh, we have to measure our business outcomes”. And so we measure things like number of deals closed, or cohort retention rates, or revenue growth, or number of blog visitors. These are all output metrics — and while they are important, they are also not particularly actionable.<br><br>Amazon argues that it’s not enough to know your output metrics. In fact, they go even further, and say that you <em>shouldn’t</em> pay much attention to your output metrics; you should pay attention to the controllable input metrics that you <em>know</em> will affect those output metrics. It is a very different thing when, say, your customer support team knows that their performance bonuses are tied to a combination of NPS and ‘% of support tickets closed within 3 days’. If you have clearly demonstrated a link between the former and the latter, then everyone on that team would be incentivised to come up with process improvements to bring that % up!
<footer>@commoncog <cite><a href="https://commoncog.com/blog/working-backwards/">https://commoncog.com/blog/working-backwards/</a></cite></footer>
</blockquote>
<script note="" src="https://cdn.jsdelivr.net/gh/Blogger-Peer-Review/quotebacks@1/quoteback.js"></script>

Input metrics are like measuring the left hand side of the equation! You're measuring the things that supposedly impact the revenue. Today's measure of revenue is not a good measure of tomorrow's revenue - input metrics are better.

Interestingly - it's quite hard to find the right input metrics, it's not always obvious exactly what input metrics *actually influence future revenue*.

<blockquote class="quoteback" darkmode="" data-title="This%20is%20How%20Amazon%20Measures%20Itself" data-author="@holistics_bi" cite="https://www.holistics.io/blog/how-amazon-measures/">
When we realized that the teams had chosen the wrong input metric—which was revealed via the WBR process—we changed the metric to reflect consumer demand instead. Over multiple WBR meetings, we asked ourselves, “If we work to change this selection metric, as currently defined, will it result in the desired output?” As we gathered more data and observed the business, this particular selection metric evolved over time from<br><br>- number of detail pages, which we refined to<br><br>- number of detail page views (you don't get credit for a new detail page if customers don't view it), which then became<br><br>- the percentage of detail page views where the products were in stock (you don't get credit if you add items but can't keep them in stock), which was ultimately finalized as<br><br>- the percentage of detail page views where the products were in stock and immediately ready for two-day shipping, which ended up being called 'Fast Track In Stock'.
<footer>@holistics_bi <cite><a href="https://www.holistics.io/blog/how-amazon-measures/">https://www.holistics.io/blog/how-amazon-measures/</a></cite></footer>
</blockquote>
<script note="" src="https://cdn.jsdelivr.net/gh/Blogger-Peer-Review/quotebacks@1/quoteback.js"></script>

Metrics as control vs metrics as discovery. 

Per working backwards, metrics are expected to change as you learn what is the best input metric to achieve your goal.

Here's some 


📈📈📈

Dashboards are a power struggle.

I see teams frustrated that dashboards don't accurately reflect effort / care. So change the dashboard.

Example: Reviews content grading project. Allows you to score content in a measurably subjective way. Allows you to embed expertise in your model. 


Real examples:
- Early stage startup putting a data studio dashboard together. Not just metrics, but measures. e.g. volume of content produced us vs competitors
- International content business with tons of data but without a good single source of truth. How to incorporate some form of explanatory power in there? Not just "what has happened" but also "what is happening"

We think of dashboards and scoreboards as being a form of command and control. A way to create legibility for the organization for the exeuctives. But.. in my experiuence, especially for abstract work like SEO

📈📈📈

I love the phrase "player scorebaord vs coach scoreboard". It reminds me that every single dashboard is, implicitly, an exercise in incentive design. By choosing what goes into the dashboard you're emphasizing what's important and what's not.

The medium is the message, you manage what you measure etc etc.

So I guess you need to be careful

📈📈📈

One of the things I love most about Google Data Studio is that it implicitly encourages *layout* as a primary activity. Yes you can style a spreadsheet but people rarely do (at least not well) whereas Data Studio is kind of saying - look this is a blank canvas and you get to say what's important and how things should be in relation to each other.

📈📈📈

Narrative dashboards. When I worked at the Google Creative Lab there was a weekly business review. Every project had to submit a 140-character status update. The executive team then scanned the list of project status updates and decided where to focus from there. Clicking on a status update opened up the full project view.

For a set of projects entirely divorced from metrics (no one cared about timeline, budget or hours at Google...) narrative dashboards created a sense of "what's going on" for the executive team.



📈📈📈

Let's take some examples for content based businesses:

- Doesn't give you cohort analysis for your content. Ideally you'd be able to view content performance normalized to "days since published" so you can see whether a piece of content is over/under perfomring
- Doesn't naturally give you content segemnetation (except via URL folder). 
- 

--

https://www.eugenewei.com/blog/2017/11/13/remove-the-legend
https://commoncog.com/blog/working-backwards/#controllable-input-metrics
https://www.holistics.io/blog/how-amazon-measures/

--

https://doubleloop.app/

Strategy articulated as a set of dependencies

--

They look at metrics, but not 


Curious case of lacking dashboards.

They look at metrics but very little 

Quote from seeing like a state
Quote from working backwards (input metrics)

https://www.franklincovey.com/the-4-disciplines/discipline-3-scoreboard/
Players scoreboard vs Coach scoreboard

---

Orbital stakeholders and permeable organizations - more dashboards are going to become public by default?

If quarterly earning reports are the tradfi way of doing things, a realtime dashboard is the defi way of doing it? Verified public transactions and data. Though of course that's going to measure what has happened - what would a public dashboard of input metrics look like?

DAOs that tokenize input metrics will have a leg up - they'll naturally be able to create dashboards that combine input and output metrics. Reality might not be so easy of course.

📈📈📈

User quotes / user research as a dashbaord vs as a one-time activity.

Amazon combines data with anecdotes to tell the whole story. The most interesting aspect of Amazon’s metrics deck, however, is their use of anecdotes. The authors write:

Amazon employs many techniques to ensure that anecdotes reach the teams that own and operate a service. One example is a program called the Voice of the Customer. The customer service department routinely collects and summarizes customer feedback and presents it during the WBR, though not necessarily every week. The chosen feedback does not always reflect the most commonly received complaint, and the CS department has wide latitude on what to present. When the stories are read at the WBR, they are often painful to hear because they highlight just how much we let customers down. But they always provide a learning experience and an opportunity for us to improve.

📈📈📈

So, to sum up some meanderings - here's some tensions of executive level dashboards. None are right or wrong, but useful ways to think about how you're setting up your dashbaords:

### 1. Qualitative / Quantitative

Is your dashboard raw data or is there some post-processing? Are you using expertise to create gradings or analysis on top of the data? Is there a voice of the customer segment for your reporting?

### 2. Input / Output

Are you reporting only on what has already happened or are you showing what's happening now also? Obviously you need both, but in my experience companies rely too heavily on output metrics.

### 3. Flexible / Fixed

How often do you update the metrics you're reporting on? Are you explicitly *designing* your metrics to be updated? What's your feedback loop for checking that inputs lead to the right outputs?

### 4. Open / Closed

Who gets to see the dashboard? How do you ensure that everyone is on the same page? Are you capturing the potential value in a world of orbital stakeholders from opening up the dashboard to a wider set of people?

📈📈📈

One thing I'm confused about: the power dynamics around dashboards. Executives I work with are often frustrated at not being to see what's going on inside their own organization. And yet they also don't invest the time to improve their own dashbaords. A curious juxtaposition and one I'm still trying to untangle - are executives afraid that imposing new dashboards will be seen as new forms of surveillance or micromangging? Is it that executives don't really understand what actually moves the needle - i.e. what a good input metric would be?

I think maybe dashboards are a representation of power, so people like to keep them neutral. Maybe there's something to that.